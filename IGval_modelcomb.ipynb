{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f351ec1",
   "metadata": {},
   "source": [
    "## Toy project (IG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048ed9f1",
   "metadata": {},
   "source": [
    "### Step 1. Module import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab9e831",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c685de",
   "metadata": {},
   "source": [
    "### Step 2. Set configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422c4478",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "# data parameter\n",
    "N_SAMPLES=4000\n",
    "N_FEATURES=1000\n",
    "N_INFORM=80\n",
    "N_REDUN=40\n",
    "\n",
    "# AE\n",
    "Z_DIM=64\n",
    "AE_HIDDEN=256\n",
    "AE_EPOCHS=100\n",
    "AE_BATCH=256\n",
    "AE_LR=1e-4\n",
    "AE_patience=20\n",
    "AE_stale=0\n",
    "\n",
    "# Transformer shape\n",
    "TOKENS=64\n",
    "D_MODEL=256\n",
    "\n",
    "device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29c820f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaf8258",
   "metadata": {},
   "source": [
    "### Step 3. Dataset preparation and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "79694442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make pseudo DEG dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=N_SAMPLES, n_features=N_FEATURES,\n",
    "    n_informative=N_INFORM, n_redundant=N_REDUN, n_repeated=0, # n_redundant: mimicking colinear gene expression\n",
    "    n_classes=2, class_sep=2.2, flip_y=0.01, random_state=SEED\n",
    ") # flip the label to prevent overfitting\n",
    "\n",
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.int64)\n",
    "\n",
    "# Train (0.6), val (0.1), test(0.2) split\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, stratify=y, random_state=SEED\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_trainval, y_trainval, test_size=0.1/0.8, stratify=y_trainval, random_state=SEED\n",
    ")\n",
    "\n",
    "# Scaling\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train).astype(np.float32)\n",
    "X_val = scaler.transform(X_val).astype(np.float32)\n",
    "X_test = scaler.transform(X_test).astype(np.float32)\n",
    "\n",
    "# Load data\n",
    "train_loader = DataLoader(TensorDataset(torch.from_numpy(X_train)), batch_size=AE_BATCH, shuffle=True, drop_last=False)\n",
    "val_loader = DataLoader(TensorDataset(torch.from_numpy(X_val)), batch_size=AE_BATCH, shuffle=True, drop_last=False)\n",
    "test_loader = DataLoader(TensorDataset(torch.from_numpy(X_test)), batch_size=AE_BATCH, shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b99bfc19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 1000)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9ffff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos=1998, neg=2002, n=4000, pos_ratio=0.4995, neg/pos=1.00\n"
     ]
    }
   ],
   "source": [
    "# Check dataset imbalance\n",
    "pos = int((y==1).sum())\n",
    "neg = int((y==0).sum())\n",
    "n = len(y)\n",
    "\n",
    "pos_ratio = pos / n\n",
    "imbalance_ratio = neg / max(pos, 1)\n",
    "print(f\"pos={pos}, neg={neg}, n={n}, pos_ratio={pos_ratio:.4f}, neg/pos={imbalance_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0fe90e",
   "metadata": {},
   "source": [
    "### Step 4. Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9fc47834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AE] 001/100 train_recon=1.0041 val_recon=1.0038\n",
      "[AE] 005/100 train_recon=0.9964 val_recon=0.9992\n",
      "[AE] 010/100 train_recon=0.9874 val_recon=0.9932\n",
      "[AE] 015/100 train_recon=0.9715 val_recon=0.9816\n",
      "[AE] 020/100 train_recon=0.9499 val_recon=0.9668\n",
      "[AE] 025/100 train_recon=0.9320 val_recon=0.9559\n",
      "[AE] 030/100 train_recon=0.9182 val_recon=0.9479\n",
      "[AE] 035/100 train_recon=0.9072 val_recon=0.9418\n",
      "[AE] 040/100 train_recon=0.8981 val_recon=0.9370\n",
      "[AE] 045/100 train_recon=0.8906 val_recon=0.9333\n",
      "[AE] 050/100 train_recon=0.8842 val_recon=0.9304\n",
      "[AE] 055/100 train_recon=0.8788 val_recon=0.9283\n",
      "[AE] 060/100 train_recon=0.8742 val_recon=0.9266\n",
      "[AE] 065/100 train_recon=0.8702 val_recon=0.9253\n",
      "[AE] 070/100 train_recon=0.8668 val_recon=0.9243\n",
      "[AE] 075/100 train_recon=0.8637 val_recon=0.9237\n",
      "[AE] 080/100 train_recon=0.8609 val_recon=0.9232\n",
      "[AE] 085/100 train_recon=0.8585 val_recon=0.9229\n",
      "[AE] 090/100 train_recon=0.8562 val_recon=0.9228\n",
      "[AE] 095/100 train_recon=0.8541 val_recon=0.9227\n",
      "[AE] 100/100 train_recon=0.8522 val_recon=0.9227\n",
      "[AE] Test reconstruction=0.9225\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder model structure\n",
    "class AE(nn.Module):\n",
    "    def __init__(self, in_dim, z_dim, hidden):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(nn.Linear(in_dim, hidden), nn.ReLU(), nn.Linear(hidden, z_dim))\n",
    "        self.dec = nn.Sequential(nn.Linear(z_dim, hidden), nn.ReLU(), nn.Linear(hidden, in_dim))\n",
    "    def forward(self, x):\n",
    "        z = self.enc(x)\n",
    "        x_hat = self.dec(z)\n",
    "        return x_hat, z\n",
    "\n",
    "ae = AE(in_dim=N_FEATURES, z_dim=Z_DIM, hidden=AE_HIDDEN).to(device)\n",
    "opt = torch.optim.Adam(ae.parameters(), lr=AE_LR)\n",
    "crit = nn.MSELoss()\n",
    "\n",
    "best_val = float('inf')\n",
    "best_state = None\n",
    "\n",
    "# Train autoencoder\n",
    "for ep in range(1, AE_EPOCHS+1):\n",
    "    # train\n",
    "    ae.train()\n",
    "    total=0.0\n",
    "    n=0\n",
    "    for (xb,) in train_loader:\n",
    "        xb = xb.to(device)\n",
    "        opt.zero_grad()\n",
    "        xhat, _ = ae(xb)\n",
    "        loss = crit(xhat, xb)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        total += loss.item()*xb.size(0)\n",
    "        n += xb.size(0)\n",
    "    train_loss = total / n\n",
    "\n",
    "    # validate\n",
    "    ae.eval()\n",
    "    with torch.no_grad():\n",
    "        vt = 0.0\n",
    "        vn = 0\n",
    "        for (xb,) in val_loader:\n",
    "            xb = xb.to(device)\n",
    "            x_hat, _ = ae(xb)\n",
    "            vloss = crit(x_hat, xb)\n",
    "            vt += vloss.item() * xb.size(0)\n",
    "            vn += xb.size(0)\n",
    "        val_loss = vt / vn\n",
    "\n",
    "    if ep%5==0 or ep==1 or ep==AE_EPOCHS:\n",
    "        print(f\"[AE] {ep:03d}/{AE_EPOCHS} train_recon={train_loss:.4f} val_recon={val_loss:.4f}\")\n",
    "\n",
    "    # early stopping\n",
    "    if val_loss + 1e-6 < best_val:\n",
    "        best_val = val_loss\n",
    "        best_state = deepcopy(ae.state_dict())\n",
    "        AE_stale = 0\n",
    "    else:\n",
    "        AE_stale += 1\n",
    "        if AE_stale >= AE_patience:\n",
    "            print(f\"Early stopping at epoch {ep} (best val {best_val:.4f})\")\n",
    "            break\n",
    "\n",
    "# Best state\n",
    "if best_state is not None:\n",
    "    ae.load_state_dict(best_state)\n",
    "\n",
    "# Test reconstruction loss\n",
    "ae.eval()\n",
    "with torch.no_grad():\n",
    "    tt = 0.0\n",
    "    tn = 0\n",
    "    for (xb,) in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        x_hat, _ = ae(xb)\n",
    "        tloss = crit(x_hat, xb)\n",
    "        tt += tloss.item() * xb.size(0)\n",
    "        tn += xb.size(0)\n",
    "    test_loss = tt / tn\n",
    "\n",
    "print(f\"[AE] Test reconstruction={test_loss:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2b2e95",
   "metadata": {},
   "source": [
    "### Step 4-1. Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c37f04c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_numpy(ae, X_np, batch_size=1024):\n",
    "    ae.eval()\n",
    "    Z = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(X_np), batch_size):\n",
    "            xb = torch.from_numpy(X_np[i:i+batch_size]).to(device)\n",
    "            _, z = ae(xb)\n",
    "            Z.append(z.cpu().numpy().astype(np.float32))\n",
    "    return np.concatenate(Z, axis=0)\n",
    "\n",
    "Z_train = encode_numpy(ae, X_train)\n",
    "Z_val = encode_numpy(ae, X_val)\n",
    "Z_test = encode_numpy(ae, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "39e0bb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 ... 1 1 0]\n",
      "2800\n"
     ]
    }
   ],
   "source": [
    "print(y_train)\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5afb4e14",
   "metadata": {},
   "source": [
    "### Step 5. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6a7acc",
   "metadata": {},
   "source": [
    "#### 5-1. Application to transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d8a737",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Transformer architecture for \n",
    "class TransformerHead(nn.Module):\n",
    "    def __init__(self, z_dim, tokens, d_model, nhead=8, dim_ff=256, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.tokens, self.d_model = tokens, d_model\n",
    "        self.embedding = nn.Linear(1, d_model)\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, nhead=nhead, dim_feedforward=dim_ff,\n",
    "            dropout=dropout, activation=\"gelu\", batch_first=True, norm_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=4)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.out = nn.Linear(d_model, 1)\n",
    "    def forward(self, z):\n",
    "        B, latent_dim = z.shape\n",
    "        z_tokens = z.unsqueeze(-1)  # (B, tokens=latent_dim, 1)\n",
    "        x = self.embedding(z_tokens)    # (B, tokens, d_model)\n",
    "        h = self.encoder(x) # (B, tokens, d_model)\n",
    "        h = self.norm(h).mean(1)    # (B, d_model)\n",
    "        base = self.out(h)  # (B, 1)\n",
    "        # simple interaction on two coords to induce rare bursts\n",
    "        return base # (B, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ed65e635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 |Train Loss: 0.7415 | Train Acc: 0.5029 | train AUROC: 0.5043 |Val Loss: 0.7372 | Val Acc: 0.5000 | val AUROC: 0.4752\n",
      "Epoch 2/50 |Train Loss: 0.6987 | Train Acc: 0.4968 | train AUROC: 0.4896 |Val Loss: 0.6937 | Val Acc: 0.5000 | val AUROC: 0.5078\n",
      "Epoch 3/50 |Train Loss: 0.6979 | Train Acc: 0.5100 | train AUROC: 0.5070 |Val Loss: 0.6932 | Val Acc: 0.5000 | val AUROC: 0.5241\n",
      "Epoch 4/50 |Train Loss: 0.6956 | Train Acc: 0.4893 | train AUROC: 0.4869 |Val Loss: 0.6952 | Val Acc: 0.5000 | val AUROC: 0.5338\n",
      "Epoch 5/50 |Train Loss: 0.6955 | Train Acc: 0.4957 | train AUROC: 0.4919 |Val Loss: 0.6930 | Val Acc: 0.4975 | val AUROC: 0.5432\n",
      "Epoch 6/50 |Train Loss: 0.6996 | Train Acc: 0.4893 | train AUROC: 0.4926 |Val Loss: 0.6936 | Val Acc: 0.5000 | val AUROC: 0.5411\n",
      "Epoch 7/50 |Train Loss: 0.7007 | Train Acc: 0.5000 | train AUROC: 0.4949 |Val Loss: 0.6949 | Val Acc: 0.5000 | val AUROC: 0.5417\n",
      "Epoch 8/50 |Train Loss: 0.6977 | Train Acc: 0.4800 | train AUROC: 0.4773 |Val Loss: 0.6957 | Val Acc: 0.5000 | val AUROC: 0.5443\n",
      "Epoch 9/50 |Train Loss: 0.6949 | Train Acc: 0.4818 | train AUROC: 0.4767 |Val Loss: 0.6931 | Val Acc: 0.5000 | val AUROC: 0.5374\n",
      "Epoch 10/50 |Train Loss: 0.6935 | Train Acc: 0.5068 | train AUROC: 0.4999 |Val Loss: 0.7037 | Val Acc: 0.5000 | val AUROC: 0.5186\n",
      "Epoch 11/50 |Train Loss: 0.6935 | Train Acc: 0.5175 | train AUROC: 0.5220 |Val Loss: 0.6944 | Val Acc: 0.5000 | val AUROC: 0.5290\n",
      "Epoch 12/50 |Train Loss: 0.6937 | Train Acc: 0.4911 | train AUROC: 0.4929 |Val Loss: 0.6932 | Val Acc: 0.5000 | val AUROC: 0.5265\n",
      "Epoch 13/50 |Train Loss: 0.6936 | Train Acc: 0.4939 | train AUROC: 0.4784 |Val Loss: 0.6931 | Val Acc: 0.5175 | val AUROC: 0.5334\n",
      "Epoch 14/50 |Train Loss: 0.6941 | Train Acc: 0.4868 | train AUROC: 0.4922 |Val Loss: 0.6950 | Val Acc: 0.5000 | val AUROC: 0.5389\n",
      "Epoch 15/50 |Train Loss: 0.6937 | Train Acc: 0.4939 | train AUROC: 0.4902 |Val Loss: 0.6931 | Val Acc: 0.5000 | val AUROC: 0.5236\n",
      "Epoch 16/50 |Train Loss: 0.6935 | Train Acc: 0.4968 | train AUROC: 0.4953 |Val Loss: 0.6932 | Val Acc: 0.5000 | val AUROC: 0.5381\n",
      "Epoch 17/50 |Train Loss: 0.6933 | Train Acc: 0.5036 | train AUROC: 0.5002 |Val Loss: 0.6930 | Val Acc: 0.5025 | val AUROC: 0.5338\n",
      "Epoch 18/50 |Train Loss: 0.6934 | Train Acc: 0.4971 | train AUROC: 0.4996 |Val Loss: 0.6931 | Val Acc: 0.5000 | val AUROC: 0.5371\n",
      "Epoch 19/50 |Train Loss: 0.6932 | Train Acc: 0.5032 | train AUROC: 0.4943 |Val Loss: 0.6930 | Val Acc: 0.5000 | val AUROC: 0.5373\n",
      "Epoch 20/50 |Train Loss: 0.6934 | Train Acc: 0.5014 | train AUROC: 0.5064 |Val Loss: 0.6933 | Val Acc: 0.5000 | val AUROC: 0.5235\n",
      "Epoch 21/50 |Train Loss: 0.6935 | Train Acc: 0.4989 | train AUROC: 0.4948 |Val Loss: 0.6934 | Val Acc: 0.5000 | val AUROC: 0.5084\n",
      "Epoch 22/50 |Train Loss: 0.6935 | Train Acc: 0.4939 | train AUROC: 0.4949 |Val Loss: 0.6931 | Val Acc: 0.5150 | val AUROC: 0.5153\n",
      "Epoch 23/50 |Train Loss: 0.6937 | Train Acc: 0.5039 | train AUROC: 0.5013 |Val Loss: 0.6934 | Val Acc: 0.5000 | val AUROC: 0.5143\n",
      "Epoch 24/50 |Train Loss: 0.6941 | Train Acc: 0.5007 | train AUROC: 0.4924 |Val Loss: 0.6932 | Val Acc: 0.5000 | val AUROC: 0.5107\n",
      "Epoch 25/50 |Train Loss: 0.6939 | Train Acc: 0.5050 | train AUROC: 0.5020 |Val Loss: 0.6933 | Val Acc: 0.5000 | val AUROC: 0.5103\n",
      "Epoch 26/50 |Train Loss: 0.6937 | Train Acc: 0.4893 | train AUROC: 0.4946 |Val Loss: 0.6937 | Val Acc: 0.5000 | val AUROC: 0.5099\n",
      "Epoch 27/50 |Train Loss: 0.6932 | Train Acc: 0.5050 | train AUROC: 0.5081 |Val Loss: 0.6935 | Val Acc: 0.5000 | val AUROC: 0.4995\n",
      "Epoch 28/50 |Train Loss: 0.6933 | Train Acc: 0.4986 | train AUROC: 0.5061 |Val Loss: 0.6933 | Val Acc: 0.5000 | val AUROC: 0.5071\n",
      "Epoch 29/50 |Train Loss: 0.6933 | Train Acc: 0.4993 | train AUROC: 0.5155 |Val Loss: 0.6932 | Val Acc: 0.4975 | val AUROC: 0.5005\n",
      "Epoch 30/50 |Train Loss: 0.6932 | Train Acc: 0.4943 | train AUROC: 0.5012 |Val Loss: 0.6931 | Val Acc: 0.5050 | val AUROC: 0.5019\n",
      "Epoch 31/50 |Train Loss: 0.6931 | Train Acc: 0.5100 | train AUROC: 0.5083 |Val Loss: 0.6930 | Val Acc: 0.5175 | val AUROC: 0.5093\n",
      "Epoch 32/50 |Train Loss: 0.6934 | Train Acc: 0.5021 | train AUROC: 0.4950 |Val Loss: 0.6931 | Val Acc: 0.5000 | val AUROC: 0.5039\n",
      "Epoch 33/50 |Train Loss: 0.6937 | Train Acc: 0.4839 | train AUROC: 0.4857 |Val Loss: 0.6931 | Val Acc: 0.5225 | val AUROC: 0.5074\n",
      "Epoch 34/50 |Train Loss: 0.6933 | Train Acc: 0.5039 | train AUROC: 0.5012 |Val Loss: 0.6931 | Val Acc: 0.5200 | val AUROC: 0.5091\n",
      "Epoch 35/50 |Train Loss: 0.6932 | Train Acc: 0.5075 | train AUROC: 0.5081 |Val Loss: 0.6932 | Val Acc: 0.5000 | val AUROC: 0.5082\n",
      "Epoch 36/50 |Train Loss: 0.6938 | Train Acc: 0.5036 | train AUROC: 0.4969 |Val Loss: 0.6931 | Val Acc: 0.5000 | val AUROC: 0.5068\n",
      "Epoch 37/50 |Train Loss: 0.6936 | Train Acc: 0.4879 | train AUROC: 0.4911 |Val Loss: 0.6932 | Val Acc: 0.5000 | val AUROC: 0.5122\n",
      "Epoch 38/50 |Train Loss: 0.6934 | Train Acc: 0.5007 | train AUROC: 0.4922 |Val Loss: 0.6932 | Val Acc: 0.5000 | val AUROC: 0.5081\n",
      "Epoch 39/50 |Train Loss: 0.6934 | Train Acc: 0.4939 | train AUROC: 0.4909 |Val Loss: 0.6932 | Val Acc: 0.5000 | val AUROC: 0.5395\n",
      "Epoch 40/50 |Train Loss: 0.6932 | Train Acc: 0.5039 | train AUROC: 0.4971 |Val Loss: 0.6931 | Val Acc: 0.5025 | val AUROC: 0.5089\n",
      "Epoch 41/50 |Train Loss: 0.6932 | Train Acc: 0.4950 | train AUROC: 0.4971 |Val Loss: 0.6931 | Val Acc: 0.5150 | val AUROC: 0.5076\n",
      "Epoch 42/50 |Train Loss: 0.6936 | Train Acc: 0.4850 | train AUROC: 0.4869 |Val Loss: 0.6932 | Val Acc: 0.5000 | val AUROC: 0.5092\n",
      "Epoch 43/50 |Train Loss: 0.6932 | Train Acc: 0.4975 | train AUROC: 0.4958 |Val Loss: 0.6931 | Val Acc: 0.4925 | val AUROC: 0.5072\n",
      "Epoch 44/50 |Train Loss: 0.6936 | Train Acc: 0.5050 | train AUROC: 0.4961 |Val Loss: 0.6931 | Val Acc: 0.5000 | val AUROC: 0.5068\n",
      "Epoch 45/50 |Train Loss: 0.6933 | Train Acc: 0.4846 | train AUROC: 0.4991 |Val Loss: 0.6934 | Val Acc: 0.5000 | val AUROC: 0.5066\n",
      "Epoch 46/50 |Train Loss: 0.6932 | Train Acc: 0.4979 | train AUROC: 0.5010 |Val Loss: 0.6931 | Val Acc: 0.4950 | val AUROC: 0.5052\n",
      "Epoch 47/50 |Train Loss: 0.6931 | Train Acc: 0.5046 | train AUROC: 0.5111 |Val Loss: 0.6942 | Val Acc: 0.5000 | val AUROC: 0.5061\n",
      "Epoch 48/50 |Train Loss: 0.6933 | Train Acc: 0.5086 | train AUROC: 0.5069 |Val Loss: 0.6934 | Val Acc: 0.5150 | val AUROC: 0.5049\n",
      "Epoch 49/50 |Train Loss: 0.6931 | Train Acc: 0.5079 | train AUROC: 0.5125 |Val Loss: 0.6931 | Val Acc: 0.4925 | val AUROC: 0.5036\n",
      "Epoch 50/50 |Train Loss: 0.6929 | Train Acc: 0.5111 | train AUROC: 0.5143 |Val Loss: 0.6933 | Val Acc: 0.5100 | val AUROC: 0.5043\n",
      "Test Accuracy: 0.5075 | Test AUROC: 0.5216\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = TensorDataset(torch.from_numpy(Z_train).float(), torch.from_numpy(y_train).float())\n",
    "val_dataset   = TensorDataset(torch.from_numpy(Z_val).float(), torch.from_numpy(y_val).float())\n",
    "test_dataset = TensorDataset(torch.from_numpy(Z_test).float(), torch.from_numpy(y_test).float())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Set optimizer and loss function\n",
    "latent_dim = Z_train.shape[1]\n",
    "TFmodel = TransformerHead(z_dim=latent_dim, tokens= latent_dim, d_model=128, nhead=8).to(device)\n",
    "optimizer = optim.Adam(TFmodel.parameters(), lr=5e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Training\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    TFmodel.train()\n",
    "    train_loss = 0.0\n",
    "    all_preds_train = []\n",
    "    all_labels_train = []\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = TFmodel(xb)\n",
    "        yb = yb.unsqueeze(1)\n",
    "        loss = criterion(y_pred, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * xb.size(0)\n",
    "\n",
    "        prob = torch.sigmoid(y_pred)\n",
    "        all_preds_train.append(prob.cpu())\n",
    "        all_labels_train.append(yb.cpu())\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    all_preds_train = torch.cat(all_preds_train).detach().numpy()\n",
    "    all_labels_train = torch.cat(all_labels_train).detach().numpy()\n",
    "    train_acc = accuracy_score(all_labels_train, all_preds_train > 0.5)\n",
    "    train_auc = roc_auc_score(all_labels_train, all_preds_train)\n",
    "\n",
    "    # validation\n",
    "    TFmodel.eval()\n",
    "    val_loss = 0.0\n",
    "    all_preds_val = []\n",
    "    all_labels_val = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            y_pred = TFmodel(xb)\n",
    "            yb = yb.unsqueeze(1)\n",
    "            loss = criterion(y_pred, yb)\n",
    "            val_loss += loss.item() * xb.size(0)\n",
    "\n",
    "            prob = torch.sigmoid(y_pred)\n",
    "            all_preds_val.append(prob.cpu())\n",
    "            all_labels_val.append(yb.cpu())\n",
    "\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    all_preds_val = torch.cat(all_preds_val).detach().numpy()\n",
    "    all_labels_val = torch.cat(all_labels_val).detach().numpy()\n",
    "    val_acc = accuracy_score(all_labels_val, all_preds_val > 0.5)\n",
    "    val_auc = roc_auc_score(all_labels_val, all_preds_val)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} |\"\n",
    "          f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | train AUROC: {train_auc:.4f} |\"\n",
    "          f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f} | val AUROC: {val_auc:.4f}\")\n",
    "\n",
    "# Test evaluation metrics\n",
    "TFmodel.eval()\n",
    "all_preds_test = []\n",
    "all_labels_test = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        y_pred = TFmodel(xb)\n",
    "        yb = yb.unsqueeze(1)\n",
    "        prob = torch.sigmoid(y_pred)\n",
    "        all_preds_test.append(prob.cpu())\n",
    "        all_labels_test.append(yb.cpu())\n",
    "\n",
    "all_preds_test = torch.cat(all_preds_test).detach().numpy()\n",
    "all_labels_test = torch.cat(all_labels_test).detach().numpy()\n",
    "test_acc = accuracy_score(all_labels_test, all_preds_test > 0.5)\n",
    "test_auc = roc_auc_score(all_labels_test, all_preds_test)\n",
    "\n",
    "print(f\"Test Accuracy: {test_acc:.4f} | Test AUROC: {test_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15de7690",
   "metadata": {},
   "source": [
    "#### 5-2. Application of IG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddf089b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# model rapper\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "ae.to(device).eval()\n",
    "TFmodel.to(device).eval()\n",
    "\n",
    "class XtoZ(nn.Module):\n",
    "    def __init__(self, ae):\n",
    "        super().__init__()\n",
    "        self.ae = ae\n",
    "    def forward(self, x):\n",
    "        z = self.ae.enc(x)\n",
    "        return z\n",
    "\n",
    "class ZtoY(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.transformer = model\n",
    "    def forward(self, z):\n",
    "        return self.transformer(z)  # (B,1)\n",
    "\n",
    "class XtoY(nn.Module):\n",
    "    def __init__(self, ae, model):\n",
    "        super().__init__()\n",
    "        self.ae = ae\n",
    "        self.transformer = model\n",
    "    def forward(self, x):\n",
    "        z = self.ae.enc(x)\n",
    "        return self.transformer(z)\n",
    "\n",
    "x_to_z = XtoZ(ae).to(device).eval()\n",
    "z_to_y = ZtoY(TFmodel).to(device).eval()\n",
    "x_to_y = XtoY(ae, TFmodel).to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e840eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_test=800, n_feat=1000\n"
     ]
    }
   ],
   "source": [
    "# Tensor data\n",
    "X_train_t = torch.from_numpy(X_train.astype(np.float32)).to(device)\n",
    "X_test_t  = torch.from_numpy(X_test.astype(np.float32)).to(device)\n",
    "\n",
    "n_test, n_feat = X_test_t.shape\n",
    "print(f\"n_test={n_test}, n_feat={n_feat}\")\n",
    "\n",
    "# baseline setting\n",
    "baseline_vec = X_train_t.mean(0)    # (D,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142d971f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrated Gradients\n",
    "def integrated_gradients(x_single, *, steps=128):\n",
    "    x_to_y.eval()\n",
    "    x = x_single.to(device).float()\n",
    "    baseline = baseline_vec.to(device).float()\n",
    "\n",
    "    # (steps+1, 1) alpha\n",
    "    alphas = torch.linspace(0.0, 1.0, steps+1, device=device).view(-1, 1)\n",
    "    # (steps+1, D)\n",
    "    x_interp = baseline.unsqueeze(0) + alphas * (x.unsqueeze(0) - baseline.unsqueeze(0))\n",
    "    x_interp.requires_grad_(True)\n",
    "\n",
    "    # forward\n",
    "    y = x_to_y(x_interp).squeeze(-1)    # (steps+1,)\n",
    "    y_sum = y.sum()\n",
    "\n",
    "    # backward\n",
    "    (grads,) = torch.autograd.grad(\n",
    "        y_sum, x_interp,\n",
    "        retain_graph=False,\n",
    "        create_graph=False\n",
    "    )   # (steps+1, D)\n",
    "\n",
    "    avg_grad = grads.mean(dim=0)    # (D,)\n",
    "    ig = (x - baseline) * avg_grad  # (D,)\n",
    "\n",
    "    return ig.detach().cpu().numpy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d175a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "def compute_ig_matrix(X_tensor, *, steps=128, idx_list=None):\n",
    "    if idx_list is None:\n",
    "        idx_list = np.arange(X_tensor.shape[0])\n",
    "\n",
    "    ig_list = []\n",
    "    for i in idx_list:\n",
    "        x_single = X_tensor[i]  # (D,)\n",
    "        ig = integrated_gradients(x_single, steps=steps)    # (D,)\n",
    "        ig_list.append(ig)\n",
    "\n",
    "    ig_mat = np.stack(ig_list, axis=0)  # (N_used, D)\n",
    "    return ig_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2f78039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref IG shape: (800, 1000)\n",
      "approx IG shape: (800, 1000)\n"
     ]
    }
   ],
   "source": [
    "# reference IG vs approximation IG\n",
    "steps_ref    = 256\n",
    "steps_approx = 16\n",
    "\n",
    "idx_all = np.arange(n_test)\n",
    "\n",
    "ig_ref = compute_ig_matrix(X_test_t, steps=steps_ref, idx_list=idx_all)\n",
    "print(\"ref IG shape:\", ig_ref.shape)\n",
    "\n",
    "ig_apx = compute_ig_matrix(X_test_t, steps=steps_approx, idx_list=idx_all)\n",
    "print(\"approx IG shape:\", ig_apx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5a6b9aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation function\n",
    "def prep(v, use_abs=True, eps=1e-12):\n",
    "    v = np.asarray(v).ravel()\n",
    "    if use_abs:\n",
    "        v = np.abs(v)\n",
    "    return np.clip(v, eps, None)\n",
    "\n",
    "def corr_report(ref, apx, desc=\"\", top_frac=0.1):\n",
    "    ref_flat = prep(ref)\n",
    "    apx_flat = prep(apx)\n",
    "\n",
    "    # global\n",
    "    sp_global = spearmanr(ref_flat, apx_flat).correlation\n",
    "    pe_global = pearsonr(ref_flat, apx_flat)[0]\n",
    "\n",
    "    # global gene importance\n",
    "    ref_gene = prep(np.mean(np.abs(ref), axis=0))\n",
    "    apx_gene = prep(np.mean(np.abs(apx), axis=0))\n",
    "\n",
    "    # top-k mask\n",
    "    kq   = 1.0 - top_frac\n",
    "    thr  = np.quantile(ref_gene, kq)\n",
    "    mask = ref_gene >= thr\n",
    "\n",
    "    sp_gene_all = spearmanr(ref_gene, apx_gene).correlation\n",
    "    pe_gene_all = pearsonr(ref_gene, apx_gene)[0]\n",
    "\n",
    "    sp_gene_top = spearmanr(ref_gene[mask], apx_gene[mask]).correlation\n",
    "    pe_gene_top = pearsonr(ref_gene[mask], apx_gene[mask])[0]\n",
    "\n",
    "    print()\n",
    "    print(f\"=== {desc} ===\")\n",
    "    print(f\"[GLOBAL (all entries)]  Spearman={sp_global:.3f}  Pearson={pe_global:.3f}\")\n",
    "    print(f\"[GENE MEAN (all gene)] Spearman={sp_gene_all:.3f}  Pearson={pe_gene_all:.3f}\")\n",
    "    print(f\"[GENE MEAN (top {int(top_frac*100)}% gene by ref)] \"\n",
    "          f\"Spearman={sp_gene_top:.3f}  Pearson={pe_gene_top:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2211e2c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Ref IG (steps=256) vs Approx IG (steps=16) ===\n",
      "[GLOBAL (all entries)]  Spearman=0.998  Pearson=0.999\n",
      "[GENE MEAN (all gene)] Spearman=1.000  Pearson=1.000\n",
      "[GENE MEAN (top 10% gene by ref)] Spearman=0.991  Pearson=0.997\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Report\n",
    "corr_report(ig_ref, ig_apx,\n",
    "            desc=f\"Ref IG (steps={steps_ref}) vs Approx IG (steps={steps_approx})\",\n",
    "            top_frac=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
